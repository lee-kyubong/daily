{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류를 위한 텍스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어집 만들기\n",
    "voca = {}\n",
    "with open('SMSSpamCollection') as file_handle:\n",
    "    for line in file_handle: #텍스트 파일을 한줄씩 읽기\n",
    "        splits = line.split() #각 줄을 space로 구분해 list화\n",
    "        label = splits[0] #문장의 첫 시작은 라벨링\n",
    "        text = splits[1:] \n",
    "        \n",
    "        for word in text:\n",
    "            lower = word.lower()\n",
    "            if not lower in voca: ###lower in voca?: 맞다. 빈 voca dict\n",
    "                voca[lower] = len(voca) #기존 사전에 해당 단어가 없으면 고유번호 할당 {'단어': 고유번호}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어집 활용해 '단어 빈도 피처' 생성\n",
    "import numpy as np\n",
    "\n",
    "features = []\n",
    "with open('SMSSpamCollection') as file_handle:\n",
    "    for line in file_handle: #한 줄에 SMS 데이터 하나(문서 하나)\n",
    "        feature = np.zeros(len(voca))\n",
    "        splits = line.split()\n",
    "        text = splits[1:] ###splits?\n",
    "        for word in text:\n",
    "            lower = word.lower()\n",
    "            feature[voca[lower]] += 1 #feature라는 벡터의 고유번호번째 0에 +1 (빈도 count)\n",
    "        \n",
    "        feature = feature / sum(feature)\n",
    "        features.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 features는 넘파이 리스트로써 각각의 넘파이 리스트가 한 문서의 피처를 담고 있음.\n",
    "각 단어가 몇 번씩 나왔는지 np.zero 넘파이 배열에 기록하고, 한 문서가 끝나면 전체 단어 개수로 각 단어의 출현 수를 나눠, 각 단어 빈도 피처값을 계산. 즉, 저장된 하나의 넘파이 배열은 문서 하나(SMS 하나)를 뜻함. -> 0인 항목들이 대부분이므로, _sparse matrix_화 시킬 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#레이블 처리\n",
    "labels = []\n",
    "with open('SMSSpamCollection') as file_handle:\n",
    "    for line in file_handle:\n",
    "        splits = line.split()\n",
    "        label = splits[0]\n",
    "        if label == 'spam':\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13627"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 5574)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위까지 feature와 label의 수작업 준비. sklearn의 CountVectorizer, TfidTransformer 통한 피처 생성과 pickle 라이브러리 활용한 내용 저장 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "spam_header = 'spam\\t'\n",
    "no_spam_header = 'ham\\t'\n",
    "documents = []\n",
    "labels = []\n",
    "\n",
    "with open('SMSSpamCollection') as file_handle:\n",
    "    for line in file_handle:\n",
    "        # 각 줄에서 레이블 부분만 떼어내고 나머지를 documents에 할당\n",
    "        if line.startswith(spam_header):\n",
    "            labels.append(1)\n",
    "            documents.append(line[len(spam_header):])\n",
    "        elif line.startswith(no_spam_header):\n",
    "            labels.append(0)\n",
    "            documents.append(line[len(no_spam_header):])\n",
    "\n",
    "vectorizer = CountVectorizer()  # 단어 횟수 피처를 만드는 클래스\n",
    "term_counts = vectorizer.fit_transform(documents)  # 문서에서 단어 횟수 counting\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "# 단어 횟수 피처에서 단어 빈도 피처를 만드는 클래스\n",
    "# tf-idf에서 idf를 생성하지 않으면 단어 빈도(term frequency)가 생성됨\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(term_counts)\n",
    "features = tf_transformer.transform(term_counts)\n",
    "\n",
    "# 처리된 파일을 저장\n",
    "with open('processed.pickle', 'wb') as file_handle:\n",
    "    pickle.dump((vocabulary, features, labels), file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.BufferedWriter"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(file_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature 이용해 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9731\n",
      "test accuracy: 0.9612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "with open('processed.pickle', 'rb') as file_handle:\n",
    "    vocabulary, features, labels = pickle.load(file_handle)\n",
    "\n",
    "# 학습-평가 데이터 나누기\n",
    "# 처음 50%를 학습으로 사용하고 나머지를 평가로 사용\n",
    "total_number = len(labels)\n",
    "middle_index = total_number//2\n",
    "train_features = features[:middle_index,:]\n",
    "train_labels = labels[:middle_index]\n",
    "test_features = features[middle_index:,:]\n",
    "test_labels = labels[middle_index:]\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_features, train_labels)\n",
    "print('train accuracy: %4.4f' % classifier.score(train_features, train_labels))\n",
    "print('test accuracy: %4.4f' % classifier.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 4.3649 word: txt\n",
      "score 4.0988 word: call\n",
      "score 3.3730 word: free\n",
      "score 2.6237 word: text\n",
      "score 2.5961 word: to\n",
      "score 2.4842 word: uk\n",
      "score 2.4744 word: www\n",
      "score 2.4241 word: stop\n",
      "score 2.4017 word: claim\n",
      "score 2.1648 word: 150p\n",
      "score 2.1367 word: or\n",
      "score 2.1299 word: service\n",
      "score 2.1253 word: from\n",
      "score 2.1086 word: my\n",
      "score 2.0982 word: mobile\n",
      "score 2.0346 word: me\n",
      "score 2.0141 word: now\n",
      "score 1.9831 word: reply\n",
      "score 1.8793 word: ur\n",
      "score 1.8430 word: prize\n"
     ]
    }
   ],
   "source": [
    "# 어떤 항목이 판별에 영향을 많이 미쳤는가?\n",
    "weights = classifier.coef_[0, :]\n",
    "pairs = []\n",
    "for index, value in enumerate(weights):\n",
    "    pairs.append( (abs(value), vocabulary[index]) )\n",
    "pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "for pair in pairs[:20]:\n",
    "    print('score %4.4f word: %s' % pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델 시스템 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyubonglee/anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: home,work,im,amp,cos,buy,wan,try,going,meet,\n",
      "topic 1: gt,lt,great,min,sure,cash,make,holiday,live,service,\n",
      "topic 2: just,got,right,lor,lol,wat,really,friends,like,told,\n",
      "topic 3: good,thanks,help,ya,heart,morning,ur,house,meeting,yes,\n",
      "topic 4: ok,love,know,want,need,come,da,tell,night,hey,\n",
      "topic 5: ur,number,hi,way,oh,sent,msg,send,cool,bt,\n",
      "topic 6: ll,later,just,sorry,did,pls,doing,yeah,like,got,\n",
      "topic 7: day,time,going,happy,ok,say,said,lor,today,went,\n",
      "topic 8: free,text,txt,stop,mobile,reply,dear,ur,www,life,\n",
      "topic 9: message,send,care,pick,gud,nice,new,year,smile,ur,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "spam_header = 'spam\\t'\n",
    "no_spam_header = 'ham\\t'\n",
    "documents = []\n",
    "\n",
    "# LDA서는 레이블 생성필요 X. 문서만 추출.\n",
    "with open('SMSSpamCollection') as file_handle:\n",
    "    for line in file_handle:\n",
    "        if line.startswith(spam_header):\n",
    "            documents.append(line[len(spam_header):])\n",
    "        elif line.startswith(no_spam_header):\n",
    "            documents.append(line[len(no_spam_header):])\n",
    "\n",
    "#LDA는 단어 빈도 피쳐가 아닌 단어가 나온 갯수가 잘 동작하므로 CountVectorizer를 사용\n",
    "#토픽 모델에 도움이 되지 않는 단어들(stop_words)을 자동으로 제거\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=2000)\n",
    "term_counts = vectorizer.fit_transform(documents)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "# 토픽 모델 학습\n",
    "topic_model = LatentDirichletAllocation(n_topics=10)\n",
    "topic_model.fit(term_counts)\n",
    "\n",
    "# 학습된 토픽들을 하나씩 출력\n",
    "topics = topic_model.components_\n",
    "for topic_id, weights in enumerate(topics):\n",
    "    print('topic %d' % topic_id, end=': ')\n",
    "    pairs = []\n",
    "    for term_id, value in enumerate(weights):\n",
    "        pairs.append( (abs(value), vocabulary[term_id]) )\n",
    "    pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "    for pair in pairs[:10]:\n",
    "        print(pair[1], end=',')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
